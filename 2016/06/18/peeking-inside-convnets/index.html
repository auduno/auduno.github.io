<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  <title>Peeking inside Convnets | Audun M Øygard</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Convolutional neural networks are used extensively for a number of image related tasks these days. Despite being very successful, they&apos;re mostly seen as &quot;black box&quot; models, since it&apos;s hard to understa">
<meta property="og:type" content="article">
<meta property="og:title" content="Peeking inside Convnets">
<meta property="og:url" content="http://auduno.github.io/2016/06/18/peeking-inside-convnets/index.html">
<meta property="og:site_name" content="Audun M Øygard">
<meta property="og:description" content="Convolutional neural networks are used extensively for a number of image related tasks these days. Despite being very successful, they&apos;re mostly seen as &quot;black box&quot; models, since it&apos;s hard to understa">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_10_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_334_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_345_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_425_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/max_activation_05.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_05_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_94_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_159_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_201_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_432_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_258_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_7_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_136_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_88_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_449_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_249_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_468_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_170_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_75_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg_filter_484_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_116_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_125_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_364_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_422_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_433_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_265_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_27_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_497_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_243_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_252_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_164_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vggs_filter_300_crop.jpg">
<meta property="og:image" content="http://auduno.github.io/images/max_activations_252.jpg">
<meta property="og:image" content="http://auduno.github.io/images/max_activations_164.jpg">
<meta property="og:image" content="http://auduno.github.io/images/max_activations_300.jpg">
<meta property="og:image" content="http://auduno.github.io/images/googlenet_4c_0411.jpg">
<meta property="og:image" content="http://auduno.github.io/images/googlenet_4c_0418.jpg">
<meta property="og:image" content="http://auduno.github.io/images/googlenet_4c_0223.jpg">
<meta property="og:image" content="http://auduno.github.io/images/googlenet_4c_0423.jpg">
<meta property="og:image" content="http://auduno.github.io/images/googlenet_4c_0390.jpg">
<meta property="og:image" content="http://auduno.github.io/images/googlenet_4c_0340.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg16_filter_13.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg16_filter_10.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg16_filter_0.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg16_filter_4.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg16_filter_17.jpg">
<meta property="og:image" content="http://auduno.github.io/images/vgg16_filter_2.jpg">
<meta property="og:updated_time" content="2016-06-18T20:30:53.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Peeking inside Convnets">
<meta name="twitter:description" content="Convolutional neural networks are used extensively for a number of image related tasks these days. Despite being very successful, they&apos;re mostly seen as &quot;black box&quot; models, since it&apos;s hard to understa">
<meta name="twitter:image" content="http://auduno.github.io/images/vgg_filter_10_crop.jpg">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
    <link rel="stylesheet" href="/css/style.css">
    
  
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-32670536-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  <link href="//netdna.bootstrapcdn.com/font-awesome/3.0/css/font-awesome.css" rel="stylesheet">
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
  <script language="javascript" type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/prettify/188.0.0/prettify.js"></script>
</head>

<body>
  <header>
  <div id="icon">
    <a href="/" class="nocolor"><p>Audun M. Øygard</p></a>
    <br/>
    <div id="socialicons">
      <a href="http://www.twitter.com/matsiyatzy"><i class="icon-twitter"></i></a>
      <a href="http://github.com/auduno"><i class="icon-github icon-large"></i></a>
      <a href="https://www.linkedin.com/in/audun-mathias-%C3%B8ygard-1a572041"><i class="icon-linkedin icon-large"></i></a>
      <a href="/atom.xml"><i class="icon-rss icon-medium" style="margin-left:2px;position:relative;top:0.1em;"></i></a>
    </div>
  </div>
  <nav id="nav">
    <p><a href="/about.html">About</a></p>
    <p><a href="/">Blog</a></p>
    <!--<p><a href="projects.html">Projects/Work</a></p>-->
  </nav>
  <div id="burger"></div>
  <div id="noburger" class="hide"></div>
</header>
  <div id="content">
    <article>


<img src="/images/header_googlenet_4c_0423.jpg" alt="image" class="headerimage">

<h3>Peeking inside Convnets</h3>


<p class="date"><time datetime="2016-06-18">18. June 2016</time></p>

<p>Convolutional neural networks are used extensively for a number of image related tasks these days. Despite being very successful, they're mostly seen as "black box" models, since it's hard to understand what happens inside the network. There are however methods to "peek inside" the convnets, and thus understand a bit more about how they work.</p>

<p>In a <a href="/2015/07/29/visualizing-googlenet-classes/">previous blogpost</a> I showed how you could use gradient ascent, with some special tricks, to make a convolutional network visualize the classes it's learnt to classify. In this post I'll show that the same technique can also be used to "peek inside the network" by visualizing what the individual units in a layer detect. To give you an idea of the results, here's some highlights of visualizations of individual units from convolutional layer 5 in the <a href="https://gist.github.com/ksimonyan/fd8800eeb36e276cd6f9#file-readme-md" target="_blank" rel="external">VGG-S</a> network:</p>

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/vgg_filter_10_crop.jpg" alt="Visualization of filter 10 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_334_crop.jpg" alt="Visualization of filter 334 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
</figure>
<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/vgg_filter_345_crop.jpg" alt="Visualization of filter 345 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_425_crop.jpg" alt="Visualization of filter 425 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<figcaption>Visualization of units 10,334,425 and 435 in convolutional layer 5 in VGG-S.</figcaption>
</figure>

<p>From top left we can pretty clearly see the head of a cocker spaniel-type dog, the head of some kind of bird, the ears of a canine, and a seaside coastline. Not all unit vizualisations are as clearly defined as these, but most nevertheless give us some interesting insights into what the individual units detect.</p>

<p>Earlier methods for figuring out what the units detect (e.g. in <a href="https://arxiv.org/abs/1311.2901" target="_blank" rel="external">Zeiler & Fergus</a>) have been to find images which maximally activate the individual units. Here's an example of the images (sampled from numerous crops of 100 000 images in the imagenet validation dataset) which give maximal activations for a specific unit in layer 5 of VGG-S:</p>

<figure style="margin-bottom:1.3em">
	<img src="/images/max_activation_05.jpg" alt="Images maximally activating filter 5 in VGG-S">
	<figcaption>Image crops maximally activating unit 5 in layer 5 VGG-S</figcaption>
</figure>

<p>While this gives us an idea of what the unit is detecting, by visualizing the same unit we can see explicitly the details the unit is focusing on. Applying this technique to the same unit as above, we can see that the unit seems to focus on the characteristic pattern on the muzzle of the dog, seemingly ignoring most other details in the image.</p>

<figure style="margin-bottom:1.3em">
	<img src="/images/vgg_filter_05_crop.jpg" alt="Visualization of filter 5 in VGG-S" style="width:200px">
	<figcaption>Visualization of unit 5 in convolutional layer 5 in VGG-S</figcaption>
</figure>

<p>We can use our visualization technique to get an overview of what all the different units in a typical layer detects. Here we've focused on convolutional layer 5 in VGG-S, which is the final convolutional layer in that specific network. Seemingly there are a large number of units that detect very specific features, such as (from top left below) forests/bushes in the background, buildings with pitched roofs, individual trees, clouds, collars, brass instruments, ship masts, bottle/jug tops, and seemingly the shoulders of people:</p>

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/vgg_filter_94_crop.jpg" alt="Visualization of filter 94 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_159_crop.jpg" alt="Visualization of filter 159 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_201_crop.jpg" alt="Visualization of filter 201 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_432_crop.jpg" alt="Visualization of filter 432 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_258_crop.jpg" alt="Visualization of filter 258 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_7_crop.jpg" alt="Visualization of filter 7 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_136_crop.jpg" alt="Visualization of filter 136 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_88_crop.jpg" alt="Visualization of filter 88 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_449_crop.jpg" alt="Visualization of filter 449 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<figcaption>Visualization of (from top left) unit 94,159,201,432,258,7,136,88 & 449 in convolutional layer 5 in VGG-S</figcaption>
</figure>

<p>What is interesting to notice, is that the network doesn't seem to have learned detailed representations of faces. In e.g. the visualization featuring the collar, the face looks more like a spooky flesh-colored blob than a face. This might be an artifact of the visualization process, but it's not entirely unlikely that the network have either not found it necessary to learn the details, or not had the capacity to learn them.</p>

<p>There also are a surprisingly large number of units that detect dog-related features. I counted somewhere around 50, out of 512 units in the layer in total, which means a surprising 10% of the network may be dedicated solely to dogs. Here's a small sample of these:</p>

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/vgg_filter_249_crop.jpg" alt="Visualization of filter 249 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_468_crop.jpg" alt="Visualization of filter 468 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
</figure>

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/vgg_filter_170_crop.jpg" alt="Visualization of filter 170 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg_filter_75_crop.jpg" alt="Visualization of filter 75 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<figcaption>Visualization of (from top left) unit 249,468,170 & 75 in convolutional layer 5 in VGG-S</figcaption>
</figure>

<p>On the other hand I could only find a single unit that clearly detected cat features (!):</p>

<figure style="margin-bottom:1.3em">
	<img src="/images/vgg_filter_484_crop.jpg" alt="Visualization of filter 484 in VGG-S" style="width:200px">
	<figcaption>Visualization of unit 484 in convolutional layer 5 in VGG-S</figcaption>
</figure>

<p>Some of the units are more general shape detectors, detecting edges, circles, corners, cones or similar:</p>

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/vggs_filter_116_crop.jpg" alt="Visualization of filter 116 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_125_crop.jpg" alt="Visualization of filter 125 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_364_crop.jpg" alt="Visualization of filter 364 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_422_crop.jpg" alt="Visualization of filter 422 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_433_crop.jpg" alt="Visualization of filter 433 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_265_crop.jpg" alt="Visualization of filter 265 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<figcaption>Visualization of (from top left) unit 116,125,364,422,433 & 265 in convolutional layer 5 in VGG-S</figcaption>
</figure>

<p>and some seem to detect textures, such as these detecting leopard fur and wood grain:

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/vggs_filter_27_crop.jpg" alt="Visualization of filter 27 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_497_crop.jpg" alt="Visualization of filter 497 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_243_crop.jpg" alt="Visualization of filter 243 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<figcaption>Visualization of (from left) unit 27,497 & 265 in convolutional layer 5 in VGG-S</figcaption>
</figure>

</p><p>Not all of the unit visualizations are so easy to interpret, such as these:</p>

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/vggs_filter_252_crop.jpg" alt="Visualization of filter 252 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_164_crop.jpg" alt="Visualization of filter 164 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vggs_filter_300_crop.jpg" alt="Visualization of filter 300 in VGG-S" style="width:200px;display:inline-block;padding-left:15px;padding-right:15px">
	<figcaption>Visualization of (from left) unit 252,164 & 300 in convolutional layer 5 in VGG-S</figcaption>
</figure>

<p>However, if we find images that maximally activate these units, we can see that they detect respectively grids and more abstract features such as out-of-focus backgrounds, and shallow-focus/macro images.</p>

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/max_activations_252.jpg" alt="Images maximally activating filter 252 in VGG-S">
	<img src="/images/max_activations_164.jpg" alt="Images maximally activating filter 164 in VGG-S">
	<img src="/images/max_activations_300.jpg" alt="Images maximally activating filter 300 in VGG-S">
	<figcaption>Images maximally activating (from top) unit 252,164 & 300 in convolutional layer 5 in VGG-S</figcaption>
</figure>

<p>Overall, this visualization of the units give us useful insight into what the units in VGG-S detect. However, VGG-S is a relatively shallow network by todays standards, with only 5 convolutional layers. What about visualizing units in deeper networks, such as VGG-16 or GoogLeNet? Unfortunately, this doesn't seem to work as well, though it gives us some interesting results. Here for instance, is a visualization of some units in convolutional layer 4c from GoogLeNet:</p>

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/googlenet_4c_0411.jpg" alt="Visualization of filter 411 in Googlenet" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/googlenet_4c_0418.jpg" alt="Visualization of filter 418 in Googlenet" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/googlenet_4c_0223.jpg" alt="Visualization of filter 223 in Googlenet" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/googlenet_4c_0423.jpg" alt="Visualization of filter 423 in Googlenet" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/googlenet_4c_0390.jpg" alt="Visualization of filter 390 in Googlenet" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/googlenet_4c_0340.jpg" alt="Visualization of filter 340 in Googlenet" style="display:inline-block;padding-left:15px;padding-right:15px">
	<figcaption>Visualization of (from top left) unit 411,418,223,423,390 & 340 in convolutional layer 4c in GoogLeNet</figcaption>
</figure>

<p>You might recognize some of these as the "puppyslugs" from DeepDream. While these visualization are more detailed than the ones we get from VGG-S, they also have a tendency to look more psychedelic and unreal. It is not completely clear why this happens, but it seems like the center of the visualization generally seems to be a good representation of what the unit detects, while the edges gives us lots of random details.</p>

<!-- 4c 418, 4a tree -->

<p>Similarly for VGG-16, the visualizations we get are much harder to interpret, though we can see in some of these that the unit seems to detect respectively some kind of dog, a theater and a brass instrument (with players as blobs).</p>

<figure style="margin-left:auto;margin-right:auto;text-align:center">
	<img src="/images/vgg16_filter_13.jpg" alt="Visualization of filter 13 in VGG-16" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg16_filter_10.jpg" alt="Visualization of filter 10 in VGG-16" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg16_filter_0.jpg" alt="Visualization of filter 0 in VGG-16" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg16_filter_4.jpg" alt="Visualization of filter 4 in VGG-16" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg16_filter_17.jpg" alt="Visualization of filter 17 in VGG-16" style="display:inline-block;padding-left:15px;padding-right:15px">
	<img src="/images/vgg16_filter_2.jpg" alt="Visualization of filter 2 in VGG-16" style="display:inline-block;padding-left:15px;padding-right:15px">
	<figcaption>Visualization of (from top left) unit 13,10,0,4,17 & 2 in convolutional layer 5 in VGG-16</figcaption>
</figure>

<p>A hypothetical reason that these visualizations doesn't work as well for deeper networks, has to do with the nature of the convolutional networks. What each convolutional layer tries to do is to be able to detect specific features, without being sensitive to irrelevant variations such as pose, lighting, partial obstruction etc. In this sense, each convolutional layer "compresses" information and throws away irrelevant details such as pose etc. This works great when doing detection, which is what the network is actually meant to do. However, when we try to run the network in reverse and generate feasible images, for each layer we have to "guess" the irrelevant structural details that have been thrown away, and as the choices made in one layer might not be coordinated with other layers, this in effect introduces some amount of "structural noise" for each layer we have to run in reverse. This might be a minor issue for networks with few layers, such as VGG-S, but as we introduce more and more layers, the cumulative "structural noise" might simply overpower the generated structure in the image, and make the image look much less like what we would recognize as e.g. a dog, and more like what we recognize as the "puppyslugs" seen in DeepDream.</p>

<p>More investigations might have to be done to tell whether this is actually the reason that visualization fails for deeper networks, but I wouldn't be surprised if this is part of the reason. Below I briefly describe the technical details of how I made these visualizations.</p>

<h4>Technical details</h4>

<p>To visualize the features, I'm using pretty much the same technique I described earlier in <a href="">this blogpost</a>, starting from a randomly initialized image, and doing gradient ascent on the image with regards to the activation of a specific unit. We also use blurring between gradient descent iterations (which is equivalent to regularization via a smoothness prior), and gradually reduce the "width" of the blur during gradient descent in order to get natural-looking images. Since units in intermediate layers actually output a grid of activations over the entire image, we choose to optimize a single point in this grid, which gives us a feature visualization corresponding to the units receptive field.</p>

<p>Another trick I also used, was to modify the network to use leaky ReLUs instead of regular ReLUs, since otherwise the gradient will usually be zero when we start from a blank image, thus hindering initial gradient ascent. Since this modification doesn't seem to have significant effect on the predictions of the network, we can assume it doesn't have a major impact on the feature visualizations.</p>

<p>I've <a href="https://github.com/auduno/deepdraw/tree/master/filter_visualizations" target="_blank" rel="external">released the code</a> I used to make these visualizations, so take a look if you want to know more details.</p>

<h4>Similar work</h4>

<p>There has been similar work on visualizing convolutional networks by e.g. Zeiler and Fergus and lately by Yosinski, Nugyen et al. In a <a href="http://arxiv.org/abs/1602.03616" target="_blank" rel="external">recent work</a> by Nguyen, they manage to visualize features very well, based on a technique they called "mean-image initialization". Since I started writing this blog post, they've also published a <a href="https://arxiv.org/abs/1605.09304" target="_blank" rel="external">new paper</a> using Generative Adversarial Networks as priors for the visualizations, which lead to far far better visualizations than the ones I've showed above. If you are interested, do take a look at their paper or <a href="https://github.com/Evolving-AI-Lab/synthesizing" target="_blank" rel="external">the code</a> they've released!</p>

<p>If you enjoyed this post, you should  <a href="https://twitter.com/matsiyatzy" target="_blank" rel="external">follow me on twitter</a>!</p>

</article>


<nav id="blognav">
  
  
    <div class="next_post">
      <a href="/2015/08/04/drawing-with-googlenet/" id="article-nav-older" class="article-nav-link-wrap">
        <img src="/css/images/next_post.png">
        <p>Drawing with GoogLeNet</p>
      </a>
    </div>
  
</nav>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>


<script>
  var disqus_shortname = 'audunotumblr';
  
  var disqus_url = 'http://auduno.github.io/2016/06/18/peeking-inside-convnets/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>



  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


  </div>
  <div id="footer">
	<div id="socialicons">
	</div>
</div>
  <script>
  		function displayMenu() {
  			document.getElementById('burger').className = "hide";
  			document.getElementById('noburger').className = "nohide";
  			document.getElementById('nav').className = "nohide";
  		}

  		function hideMenu() {
  			document.getElementById('burger').className = "nohide";
  			document.getElementById('noburger').className = "hide";
  			document.getElementById('nav').className = "hide";
  		}

  		document.getElementById('burger').onclick = displayMenu;
  		document.getElementById('noburger').onclick = hideMenu;
  	</script>
</body>
</html>